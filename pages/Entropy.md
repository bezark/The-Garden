---
publish: true
tags: concept, ref/phys
---

- Connects [[Physics]] and [[Time]] and [[Information Theory]]. Entropy is the great demise. "The universal tendency toward heat death and disorder," as Smill calls it as well as "...perhaps the grandest of all cosmic generalizations." ^[Smil, Vaclav. Energy: A Beginner's Guide (Beginner's Guides) (p. 14). Oneworld Publications. Kindle Edition.]
- It is interesting because in my conception of [[Physics]] and [[Energy]], Entropy is by FAR the most poetic concept and the one which I am most deeply familiar. Entropy was the one thing I remembered from High School physics class. It has been a concept that has caused me great personal despair for quite some time and yet has given me hope. Or rather, the role that entropy plays in the universe has provided me comfort and purpose as the antonym of life. [[Entropy is the antonym to Life]].
# Boltzmann's Equation
![[The Order of Time#^7za32r]]

This is really interesting because it is the first real description of [[Blurring]]. The amount of entropy is logarithmically connected to the number of [[microstates]] - meaning whatever the base is, it takes S exponents to get to W microstates? Is k the base?
# Wikipedia:
> **Entropy** is a scientific concept, as well as a measurable physical property that is most commonly associated with a state of disorder, randomness, or uncertainty. The term and the concept are used in diverse fields, from classical thermodynamics, where it was first recognized, to the microscopic description of nature in statistical physics, and to the principles of information theory. It has found far-ranging applications in chemistry and physics, in biological systems and their relation to life, in cosmology, economics, sociology, weather science, climate change, and information systems including the transmission of information in telecommunication.The thermodynamic concept was referred to by Scottish scientist and engineer Macquorn Rankine in 1850 with the names thermodynamic function and heat-potential. In 1865, German physicist Rudolph Clausius, one of the leading founders of the field of thermodynamics, defined it as the quotient of an infinitesimal amount of heat to the instantaneous temperature. He initially described it as transformation-content, in German Verwandlungsinhalt, and later coined the term entropy from a Greek word for transformation. Referring to microscopic constitution and structure, in 1862, Clausius interpreted the concept as meaning disgregation.A consequence of entropy is that certain processes are irreversible or impossible, aside from the requirement of not violating the conservation of energy, the latter being expressed in the first law of thermodynamics. Entropy is central to the second law of thermodynamics, which states that the entropy of isolated systems left to spontaneous evolution cannot decrease with time, as they always arrive at a state of thermodynamic equilibrium, where the entropy is highest.
>
> Austrian physicist Ludwig Boltzmann explained entropy as the measure of the number of possible microscopic arrangements or states of individual atoms and molecules of a system that comply with the macroscopic condition of the system. He thereby introduced the concept of statistical disorder and probability distributions into a new field of thermodynamics, called statistical mechanics, and found the link between the microscopic interactions, which fluctuate about an average configuration, to the macroscopically observable behavior, in form of a simple logarithmic law, with a proportionality constant, the Boltzmann constant, that has become one of the defining universal constants for the modern International System of Units (SI).
>
> In 1948, Bell Labs scientist Claude Shannon developed similar statistical concepts of measuring microscopic uncertainty and multiplicity to the problem of random losses of information in telecommunication signals. Upon John von Neumann's suggestion, Shannon named this entity of missing information in analogous manner to its use in statistical mechanics as entropy, and gave birth to the field of information theory. This description has been proposed as a universal definition of the concept of entropy.
>
> [Wikipedia](https://en.wikipedia.org/wiki/Entropy)